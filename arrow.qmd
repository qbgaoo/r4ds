---
freeze: true
---

# Arrow {#sec-arrow}

```{r}
#| echo: false

source("_common.R")
```

## 引言

CSV 文件设计得易于人类阅读。它
们是一种很好的交换格式，因为它们非常简单，并且可以被任何工具读取。但
是 CSV 文件并不是非常高效，你需要做很多工作才能将数据读入 R。在
本章中，你将了解一个强大的替代方案：[parquet](https://parquet.apache.org/) 格式，这是一种基于开放标准的大数据系统广泛使用的格式。

我们将把 parquet 文件与 [Apache Arrow](https://arrow.apache.org) 配对使用，Apache Arrow 是一个为大型数据集的高效分析和传输而设计的多语言工具箱。我
们将通过 [arrow](https://arrow.apache.org/docs/r/) 包来使用 Apache Arrow，它提供了一个 dplyr 后端，允许你使用熟悉的 dplyr 语法来分析大于内存的数据集。作
为一个额外的优势，arrow 极其快速：你将在本章后面看到一些示例。

arrow 和 dbplyr 都提供了 dplyr 后端，所以你可能会想知道什么时候使用哪一个。在
许多情况下，这个选择已经为你做出了，因为数据已经存储在数据库中或 parquet 文件中，你会想要保持现状进行工作。但
是如果你从自己的数据（可能是 CSV 文件）开始，你可以将其加载到数据库中或将其转换为 parquet。一
般来说，很难知道哪种方法效果最好，所以在分析的早期阶段，我们鼓励你尝试两者，并选择最适合你的那一种。

（特别感谢 Danielle Navarro 贡献了本章的初始版本。）

### 必要条件

在本章中，我们将继续使用 tidyverse，特别是 dplyr，但我们将它与 arrow 包结合使用，arrow 包是专门为处理大数据而设计的。

```{r setup}
#| message: false
#| warning: false
library(tidyverse)
library(arrow)
```

在本章的后面部分，我们还将看到 arrow 和 duckdb 之间的一些联系，因此我们还需要使用 dbplyr 和 duckdb。

```{r}
library(dbplyr, warn.conflicts = FALSE)
library(duckdb)
```

## 获取数据

我们首先获取一个值得使用这些工具的数据集：西雅图公共图书馆的图书借阅数据集，可以在线获取，地址为 [data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6](https://data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6)。这
个数据集包含了 41,389,465 行数据，记录了从 2005 年 4 月到 2022 年 10 月每个月每本书的借阅次数。

以下代码将为你获取数据的缓存副本。数
据是一个 9GB 的 CSV 文件，因此下载需要一些时间。我
强烈推荐使用 `curl::multi_download()` 来获取非常大的文件，因为它正是为此目的而构建的：它提供了一个进度条，并且如果下载中断，它可以恢复下载。

```{r}
#| eval: !expr "!file.exists('data/seattle-library-checkouts.csv')"
dir.create("data", showWarnings = FALSE)

curl::multi_download(
  "https://r4ds.s3.us-west-2.amazonaws.com/seattle-library-checkouts.csv",
  "data/seattle-library-checkouts.csv",
  resume = TRUE
)
```

## 打开数据集

首先来查看一下数据。由
于该文件有 9GB，足够大，因此我们可能不想将整个文件加载到内存中。一
个很好的经验法则是，你通常希望至少有数据大小两倍的内存，而许多笔记本电脑的内存容量上限为16GB。这
意味着我们要避免使用 `read_csv()`，而是使用 `arrow::open_dataset()`：

```{r open-dataset}
seattle_csv <- open_dataset(
  sources = "data/seattle-library-checkouts.csv", 
  col_types = schema(ISBN = string()),
  format = "csv"
)
```

运行这段代码时发生了什么？`o`
`pen_dataset()` 会扫描几千行数据以确定数据集的结构。`I`
`SBN` 列在前 80,000 行中包含空值，因此我们必须指定列类型以帮助 arrow 确定数据结构。一
旦 `open_dataset()` 扫描了数据，它就会记录所发现的内容并停止；只有当你特别请求时，它才会进一步读取其他行。如
果我们输出`seattle_csv`，所看到的正是这些元数据。

```{r}
seattle_csv
```

输出的第一行告诉你 `seattle_csv` 作为单个 CSV 文件存储在本地磁盘上；它只会在需要时加载到内存中。输
出的其余部分告诉你 arrow 为每一列推断出的列类型。

我们可以使用 `glimpse()` 来查看实际内容。这
会显示大约有 4100 万行和 12 列，并展示一些值。

```{r glimpse-data}
#| cache: true
seattle_csv |> glimpse()
```

我们可以开始使用 dplyr 的函数来操作这个数据集，使用 `collect()` 来强制 arrow 执行计算并返回一些数据。例
如，以下代码会告诉我们每年图书的总借阅次数：

```{r}
#| cache: true
seattle_csv |> 
  group_by(CheckoutYear) |> 
  summarise(Checkouts = sum(Checkouts)) |> 
  arrange(CheckoutYear) |> 
  collect()
```

由于使用了arrow，这段代码无论基础数据集有多大都能正常工作。但
目前的运行速度相当慢：在 Hadley 的电脑上，运行这段代码大约需要 10 秒。考
虑到我们处理的数据量，这不算太慢，但我们可以通过切换到更好的格式来使其更快。

## parquet 格式 {#sec-parquet}

为了让这些数据更易于处理，我们将切换到 parquet 文件格式，并将其拆分成多个文件。接
下来的部分将首先向你介绍 parquet 和分区，然后将我们学到的知识应用于西雅图图书馆数据。

### parquet 的优势

和 CSV 一样，parquet 用于矩形数据，但它不是任何文件编辑器都可以读取的文本格式，而是一个专门为大数据需求设计的自定义二进制格式。这
意味着：

-   parquet 文件通常比等效的 CSV 文件小。Parquet 依赖于[efficient encodings](https://parquet.apache.org/docs/file-format/data-pages/encodings/)来减小文件大小，并支持文件压缩。这有助于使 parquet 文件更快，因为从磁盘移动到内存的数据更少。
-   parquet 文件具有丰富的类型系统。正如我们在 @sec-col-types 讨论的那样，CSV 文件不提供有关列类型的任何信息。例如CSV 阅读器必须猜测 `"08-10-2022"` 是否应被解析为字符串或日期。相反，parquet 文件以将数据与其类型一起记录的方式存储数据。
-   parquet 文件是“面向列的”。这意味着它们是按列组织的，类似于 R 的数据框。与按行组织的 CSV 文件相比，这通常会导致数据分析任务具有更好的性能。
-   parquet 文件是“分块的”，这使得可以在同一时间处理文件的不同部分，如果幸运的话，甚至可以完全跳过一些块。

Parquet 文件有一个主要缺点：它们不再是“人类可读”的，即如果你使用 `readr::read_file()` 查看 Parquet 文件，你将只能看到一堆乱码。

### 分区

随着数据集变得越来越大，将所有数据存储在一个文件中变得越来越困难，而且将大型数据集拆分成多个文件通常很有用。当
这种结构化操作进行得足够智能时，这种策略可以显著提高性能，因为许多分析只需要文件的一个子集。

关于如何对你的数据集进行分区，没有严格的规定：结果将取决于你的数据、访问模式以及读取数据的系统。在
找到适合你的理想分区之前，可能需要进行一些实验。作
为粗略的指导，arrow 建议避免使用小于 20MB 和大于 2GB 的文件，并避免产生超过 10,000 个文件的分区。你
还应该尝试按你筛选的变量进行分区；如你稍后将看到的那样，这允许 arrow 仅读取相关文件，从而节省大量工作。

### 重写西雅图图书馆数据

让我们将这些想法应用于西雅图图书馆数据，以了解它们在实际操作中的表现。我
们将按 `CheckoutYear` 进行分区，因为某些分析可能只想查看最近的数据，并且按年份分区可以产生 18 个大小合理的块。

为了重写数据，我们使用 `dplyr::group_by()` 定义分区，然后使用 `arrow::write_dataset()` 将分区保存到目录中。`w`
`rite_dataset()` 有两个重要参数：一个是用于创建文件的目录，一个是将使用的文件格式。

```{r}
pq_path <- "data/seattle-library-checkouts"
```

```{r write-dataset}
#| eval: !expr "!file.exists(pq_path)"

seattle_csv |>
  group_by(CheckoutYear) |>
  write_dataset(path = pq_path, format = "parquet")
```

这大约需要一分钟来运行；正如我们稍后将看到的，这是一项初始投资，通过使未来的操作变得更快而得到回报。

让我们来看看刚刚生成了什么：

```{r show-parquet-files}
tibble(
  files = list.files(pq_path, recursive = TRUE),
  size_MB = file.size(file.path(pq_path, files)) / 1024^2
)
```

我们原本 9GB 的 CSV 文件已经被重写为 18 个 parquet 文件。这
些文件名采用了 [Apache Hive](https://hive.apache.org)项目使用的“自描述”约定。H
ive 风格的分区使用“key=value”约定来命名文件夹，所以正如你猜测的那样，名为 `CheckoutYear=2005` 的目录包含了所有 `CheckoutYear` 为 2005 的数据。每
个文件的大小在 100MB 到 300MB 之间，总大小现在大约是 4GB，略多于原始 CSV 文件大小的一半。这
是我们所期望的，因为 parquet 是一种更加高效的格式。

## 使用 dplyr 与 arrow

现在我们已经创建了这些 parquet 文件，接下来我们需要再次读取它们。我
们再次使用 `open_dataset()`，但这次我们给它提供一个目录：

```{r}
seattle_pq <- open_dataset(pq_path)
```

现在我们可以编写我们的 dplyr 管道了。例
如，我们可以计算过去五年内每个月借出的书籍总数：

```{r books-by-year-query}
query <- seattle_pq |> 
  filter(CheckoutYear >= 2018, MaterialType == "BOOK") |>
  group_by(CheckoutYear, CheckoutMonth) |>
  summarize(TotalCheckouts = sum(Checkouts)) |>
  arrange(CheckoutYear, CheckoutMonth)
```

为 arrow 数据编写 dplyr 代码的概念与 @sec-import-databases 中的dbplyr 类似：你编写 dplyr 代码，这些代码会自动转换为 Apache Arrow C++ 库能够理解的查询，然后在调用 `collect()` 时执行。如
果我们输出查询对象，我们可以看到一些关于执行时我们期望 Arrow 返回的信息：

```{r}
query
```

我们可以通过调用`collect()`来获取结果:

```{r books-by-year}
query |> collect()
```

与 dbplyr 一样，arrow 只理解某些 R 表达式，因此你可能无法编写与通常完全一样的代码。但
是，所支持的操作和函数列表相当广泛且不断扩展；您可以在`?acero`中找到当前支持的函数的完整列表。

### 性能 {#sec-parquet-fast}

让我们快速看一下从 CSV 切换到 parquet 对性能的影响。首
先，让我们计算一下当数据作为单个大型 CSV 存储时，计算 2021 年每个月借出的书籍数量所需的时间：

```{r dataset-performance-csv}
#| cache: true

seattle_csv |> 
  filter(CheckoutYear == 2021, MaterialType == "BOOK") |>
  group_by(CheckoutMonth) |>
  summarize(TotalCheckouts = sum(Checkouts)) |>
  arrange(desc(CheckoutMonth)) |>
  collect() |> 
  system.time()
```

现在，让我们使用数据集的新版本，其中西雅图图书馆的借阅数据已经被分区为 18 个较小的 parquet 文件：

```{r dataset-performance-multiple-parquet}
#| cache: true

seattle_pq |> 
  filter(CheckoutYear == 2021, MaterialType == "BOOK") |>
  group_by(CheckoutMonth) |>
  summarize(TotalCheckouts = sum(Checkouts)) |>
  arrange(desc(CheckoutMonth)) |>
  collect() |> 
  system.time()
```

大约100倍性能的提升可归因于两个因素：多文件分区和单个文件的格式：

-   分区提高了性能，因为这个查询使用 `CheckoutYear == 2021` 来筛选数据，而 arrow 足够智能，能够识别出它只需要读取 18 个 parquet 文件中的 1 个。
-   parquet 格式提高了性能，因为它将数据以可以直接读入内存的二进制格式存储。列式格式和丰富的元数据意味着 arrow 只需要读取查询中实际使用的四列（`CheckoutYear`、`MaterialType`、`CheckoutMonth` 和 `Checkouts`）。

这种性能上的巨大差异说明为什么将大型 CSV 转换为 parquet 是值得的！

### 使用duckdb与arrow

parquet 和 arrow 的最后一个优势是，通过调用`arrow::to_duckdb()`可以非常轻松地将arrow 数据集转换为 DuckDB 数据库（ 参见 @sec-import-databases ）。

There's one last advantage of parquet and arrow --- it's very easy to turn an arrow dataset into a DuckDB database (@sec-import-databases) by calling `arrow::to_duckdb()`:

```{r use-duckdb}
seattle_pq |> 
  to_duckdb() |>
  filter(CheckoutYear >= 2018, MaterialType == "BOOK") |>
  group_by(CheckoutYear) |>
  summarize(TotalCheckouts = sum(Checkouts)) |>
  arrange(desc(CheckoutYear)) |>
  collect()
```

`to_duckdb()`的巧妙之处在于，这种转换不涉及任何内存复制，并体现了 arrow 生态系统的目标：实现从一个计算环境到另一个计算环境的无缝过渡。

### 练习

1.  找出每年最受欢迎的书。
2.  西雅图图书馆系统中哪位作者的书最多？
3.  在过去的10年里，纸质书与电子书的借阅情况是如何变化的？

## 小结

在本章中，你已经初步了解了arrow包，它提供了用于处理大型磁盘上数据集的 dplyr 后端。它
可以处理 CSV 文件，但如果您将数据转换为 parquet 格式，它会快得多。P
arquet 是一种二进制数据格式，专为现代计算机上的数据分析而设计。与
CSV 相比，能够处理 parquet 文件的工具要少得多，但其分区、压缩和列式存储结构使得分析更加高效。

接下来，您将学习关于第一个非矩形数据源的知识，您将使用 tidyr 包提供的工具来处理它。我
们将重点关注来自 JSON 文件的数据，但一般原则适用于任何来源的树状数据。
