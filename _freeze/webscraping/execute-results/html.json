{
  "hash": "b546eaf6ea898ad590b228d7f5fbb5f2",
  "result": {
    "engine": "knitr",
    "markdown": "# 网页抓取 {#sec-scraping}\n\n\n::: {.cell}\n\n:::\n\n\n## 引言\n\n本章将向您介绍使用 [rvest](https://rvest.tidyverse.org) 进行网页抓取 (web scraping) 的基础知识。网\n页抓取是一种非常有用的工具，用于从网页中提取数据。一\n些网站会提供 API，这是一组结构化的 HTTP 请求，其返回JSON 格式的数据，你可以使用 @sec-rectangling 中介绍的规整化方法来处理这些数据。在\n可能的情况下，你应该使用 API[^webscraping-1]，因为它通常会提供更可靠的数据。\n然而，不幸的是，使用 Web API 编程超出了本书的范围。\n相反，我们将教授网页抓取技术，这是一种无论网站是否提供 API 都适用的技术。\n\n[^webscraping-1]: 许多流行的API已经有CRAN包来封装它们，所以请先进行一些研究！\n\n在本章中，我们首先会讨论网页抓取的伦理道德和合法性，然后再深入探讨 HTML 的基础知识。接\n下来，学习 CSS 选择器的基础知识，以定位页面上的特定元素，以及如何使用`rvest`函数从 HTML 的文本和属性中提取数据并将其导入 R 。之\n后，我们将讨论一些技巧，以确定你需要针对要抓取的页面使用哪个 CSS 选择器。最\n后，我们将通过几个案例研究和一个关于动态网站的简短讨论来结束本章。\n\n### 必要条件\n\n在本章中，我们将重点关注`rvest`提供的工具。`r`\n`vest`是`tidyverse`的一个成员，但不是核心成员，因此你需要显式地加载它。我\n们还将加载完整的`tidyverse`，因为在我们处理已抓取的数据时，它通常会很有用。\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(rvest)\n```\n:::\n\n\n## 网页抓取的道德与法律\n\n在我们开始讨论进行网页抓取所需的代码之前，我们需要讨论你是否有权合法且合乎道德地这样做。总\n的来说，这两个方面的情况都相当复杂。\n\n合法性在很大程度上取决于你住在哪里。然\n而，作为一般原则，如果数据是公开的、非个人的和真实的，那么你可能是安全的[^webscraping-2]。\n这三个因素很重要，因为它们与网站的条款和条件、个人身份信息和版权有关，我们将在下面讨论。\n\n[^webscraping-2]: 显然，我们不是律师，这也不是法律建议。但\n    这是我们阅读了大量相关话题后能够给出的最佳总结。\n\n如果数据不是公开的、非个人的或真实的，或者你是专门为了赚钱而抓取这些数据，那么你需要咨询律师。最\n重要的是，在任何情况下你都应该尊重托管你正在抓取的页面的服务器资源。这\n意味着如果你正在抓取许多页面，你应该确保在每个请求之间稍微等待一下。一\n个简单的方法是使用 Dmytro Perepolkin 的[**polite**](https://dmi3kno.github.io/polite/)包，它会自动在请求之间暂停并缓存结果，这样你就不会两次请求相同的页面。\n\n### 服务条款\n\n如果仔细观察，你会发现许多网站在页面上的某个位置包含了一个“条款和条件”或“服务条款”的链接。如\n果你仔细阅读该页面，通常会发现该网站明确禁止网页抓取。这\n些页面企业往往会进行法律上的“圈地运动”，对其提出非常宽泛的权利主张。在\n可能的情况下，尊重这些服务条款是一种礼貌，但对任何声明都应该持保留态度。\n\n美国法院普遍认为，仅仅将服务条款放在网站的页脚不足以使你受到它们的约束，例如[HiQ Labs v. LinkedIn](https://en.wikipedia.org/wiki/HiQ_Labs_v._LinkedIn)一案。一\n般来说，要受服务条款的约束，你必须采取一些明确的行动，如创建账户或勾选复选框。这\n就是为什么数据是否公开很重要；如果你不需要账户就可以访问它们，那么你不太可能受到服务条款的约束。但\n请注意，欧洲的情况则有所不同，法院认为即使你没有明确表示同意，服务条款也是有强制性的。\n\n### 个人信息\n\n即使数据是公开的，你也应该极其小心地抓取个人身份信息，如姓名、电子邮件地址、电话号码、出生日期等。欧\n洲对收集或存储此类数据有特别严格的法律 ([GDPR](https://gdpr-info.eu/))，无论你住在哪里，你都有可能陷入道德困境。例\n如在2016年有研究人员从约会网站OkCupid上抓取了约7万人的公开个人信息资料（如用户名、年龄、性别、地点等），并公开地发布了这些数据，没有任何匿名化尝试。虽\n然研究人员认为既然数据已经是公开的，这样做就没什么错，但是由于对数据集中发布的用户身份信息的伦理担忧，这项工作受到了广泛谴责。如\n果你的工作涉及抓取个人身份信息，我们强烈建议你阅读OkCupid研究[^webscraping-3]以及涉及获取和发布个人身份信息的类似研究，这些研究的伦理都存在问题。\n\n[^webscraping-3]: 《连线》杂志发表了一篇关于OkCupid研究的文章 <https://www.wired.com/2016/05/okcupid-study-reveals-perils-big-data-science>.\n\n### 版权\n\n最后，您还需要担心版权法。版\n权法很复杂，但值得一看的是[美国法律](https://www.law.cornell.edu/uscode/text/17/102)对受保护内容的描述：“\\[...\\]以任何有形表达媒介固定的原创作品\\[...\\]”。接\n着，它描述了其适用的具体类别，如文学作品、音乐作品、电影等。值\n得注意的是，数据并不受版权保护，这意味着只要你只收集事实，版权保护就不适用。(\n但请注意，欧洲有一个单独的“[sui generis](https://en.wikipedia.org/wiki/Database_right)”权利来保护数据库。)\n\n举个简单的例子，在美国，配料表和说明是不受版权保护的，所以版权不能用来保护食谱。但\n是，如果该食谱列表伴有大量新颖的文学内容，那么这些内容是受版权保护的。这\n就是为什么当你在网上寻找食谱时，之前总是有那么多内容。\n\n如果你确实需要抓取原创内容（如文本或图像），你可能仍然受到[合理使用原则](https://en.wikipedia.org/wiki/Fair_use)的保护。合\n理使用不是一个严格的规则，而是权衡了多个因素。如\n果你出于研究或非商业目的收集数据，并且仅抓取所需内容，那么合理使用原则更可能适用。\n\n## HTML基础\n\n要抓取网页，你首先需要稍微了解HTML (**H**yper**T**ext **M**arkup **L**anguage) ，一种描述网页的语言。H\nTML 代表超文本标记语言，看起来像这样：\n\n``` html\n<html>\n<head>\n  <title>Page title</title>\n</head>\n<body>\n  <h1 id='first'>A heading</h1>\n  <p>Some text &amp; <b>some bold text.</b></p>\n  <img src='myimg.png' width='100' height='100'>\n</body>\n```\n\nHTML具有由元素形成的层次结构，这些元素包括开始标签（例如，`<tag>`）、可选属性（`id='first'`）、结束标签[^webscraping-4]（如`</tag>`）和内容（开始标签和结束标签之间的所有内容）。\n\n[^webscraping-4]: 许多标签(包括`<p>`和`<li>`)不需要结束标签，但我们认为最好包含结束标签，因为这样可以更容易地查看HTML的结构。\n\n由于`<`和`>`用于开始和结束标签，因此不能直接书写它们。相\n反，你必须使用HTML转义字符`&gt;` (大于)和`&lt;` (小于)。并\n且因为这些转义字符使用了&，如果你想要一个实际的&符号，你必须将其转义为`&amp;`。虽\n然存在许多可能的HTML转义字符，但你不需要过分担心它们，因为rvest会自动为你处理它们。\n\n网页抓取之所以可能，是因为大多数包含你想要抓取的数据的页面通常都具有一致的结构。\n\n### 元素\n\nHTML有超过100个元素。其\n中最重要的是:\n\n-   每个HTML页面都必须位于`<html>`元素中，还必须包含两个子元素：`<head>`，其中包含文档元数据，如页面标题，以及`<body>`，其中包含你在浏览器中看到的内容。\n-   诸如`<h1>` (标题1)、`<section>` (部分)、`<p>` (段落) 和 `<ol>` (有序列表) 之类的块标签构成了页面的整体结构。\n-   诸如`<b>` (粗体)、`<i>` (斜体) 和 `<a>` (链接) 之类的内联标签用以格式化块标签内的文本。\n\n如果您遇到从未见过的标签，可以通过谷歌搜索来发现它的作用。另\n一个不错的起点是 [MDN Web Docs](https://developer.mozilla.org/en-US/docs/Web/HTML)，它描述了网络编程的各个方面。\n\n大多数元素都可以在开始和结束标签之间包含内容。这\n些内容可以是文本或其他元素。例\n如，以下HTML包含一段文本，其中一个词为粗体。\n\n```         \n<p>\n  Hi! My <b>name</b> is Hadley.\n</p>\n```\n\n上面的`<p>`元素有一个子元素，即`<b>`元素，`<b>`元素没有子元素，但是它有内容 (文本“name”)。子\n元素是它包含的其他元素。\n\n### 属性\n\n标签可以有命名属性，看起来像这样：`name1='value1' name2='value2'`，其中两个最重要的属性是`id`和`class`，它们与 CSS（层叠样式表）一起使用，以控制页面的视觉外观。在\n抓取页面数据时，这些属性通常很有用。属\n性也用于记录链接的目标（`<a>`元素的`href`属性）和图像的来源（`<img>`元素的`src`属性）。\n\n## 提取数据\n\n要开始抓取数据，你需要抓取的页面的URL，这通常可以从你的网页浏览器中进行复制。然\n后，使用`read_html()`函数将该页面的HTML读取到R中。这\n会返回一个`xml_document`[^webscraping-5] 对象，之后你将使用`rvest`函数来操作这个对象：\n\n[^webscraping-5]: 该类来自[xml2](https://xml2.r-lib.org)包。x\n    ml2是一种底层包，可以在其上进行构建。\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml <- read_html(\"http://rvest.tidyverse.org/\")\nhtml\n#> {html_document}\n#> <html lang=\"en\">\n#> [1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UT ...\n#> [2] <body>\\n    <a href=\"#container\" class=\"visually-hidden-focusable\">Ski ...\n```\n:::\n\n\nrevest还包含一个函数，允许你编写内联HTML。在\n本章中，我们会用一些简单的例子来讲解各种函数的工作原理。\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml <- minimal_html(\"\n  <p>This is a paragraph</p>\n  <ul>\n    <li>This is a bulleted list</li>\n  </ul>\n\")\nhtml\n#> {html_document}\n#> <html>\n#> [1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UT ...\n#> [2] <body>\\n<p>This is a paragraph</p>\\n  <ul>\\n<li>This is a bulleted lis ...\n```\n:::\n\n\n现在已经有了用R编写的HTML，是时候提取感兴趣的数据了。你\n将首先了解CSS选择器，它允许你识别感兴趣的元素和用于从中提取数据的rvest 函数。然\n后我们将简要介绍HTML表，它有一些特殊的工具。\n\n### 查找元素\n\nCSS 是层叠样式表（Cascading Style Sheets）的缩写，是一个定义HTML文档视觉样式的工具。C\nSS包含了一种在页面上选择元素的小型语言，称为CSS选择器 (CSS selectors)。C\nSS选择器定义了定位HTML元素的模式，对于抓取数据来说非常有用，因为它们提供了一种简洁的方式来描述你想要提取哪些元素。\n\n我们将在 @sec-css-selectors 更详细地讨论CSS选择器，但幸运的是，你只需要掌握以下三种选择器就可以走得很远了：\n\n-   `p` 选择所有 `<p>` 元素；\n\n-   `.title` 选择所有带有类名 \"title\" 的元素；\n\n-   `#title` 选择 `id` 属性等于 \"title\" 的元素。i\n    d 属性在文档内必须是唯一的，因此这只会选择一个元素。\n\n让我们通过一个简单的例子来尝试一下这些选择器：:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml <- minimal_html(\"\n  <h1>This is a heading</h1>\n  <p id='first'>This is a paragraph</p>\n  <p class='important'>This is an important paragraph</p>\n\")\n```\n:::\n\n\n使用`html_elements()`来查找与选择器匹配的所有元素：\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml |> html_elements(\"p\")\n#> {xml_nodeset (2)}\n#> [1] <p id=\"first\">This is a paragraph</p>\n#> [2] <p class=\"important\">This is an important paragraph</p>\nhtml |> html_elements(\".important\")\n#> {xml_nodeset (1)}\n#> [1] <p class=\"important\">This is an important paragraph</p>\nhtml |> html_elements(\"#first\")\n#> {xml_nodeset (1)}\n#> [1] <p id=\"first\">This is a paragraph</p>\n```\n:::\n\n\n另一个重要的函数是`html_element()`，它总是返回与输入相同数量的输出。如\n果你将它应用于整个文档，它将返回第一个匹配的元素：\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml |> html_element(\"p\")\n#> {html_node}\n#> <p id=\"first\">\n```\n:::\n\n\n当你使用不匹配任何元素的选择器时，`html_element()`和`html_elements()`之间有一个重要的区别。`h`\n`tml_elements()`返回一个长度为 0 的向量，而`html_element()`返回一个缺失值。这\n在稍后的内容中会变得重要。\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml |> html_elements(\"b\")\n#> {xml_nodeset (0)}\nhtml |> html_element(\"b\")\n#> {xml_missing}\n#> <NA>\n```\n:::\n\n\n### 嵌套选择\n\n在大多数情况下，你会一起使用`html_elements()`和`html_element()`。通\n常首先使用`html_elements()`来识别将成为观测值的元素，然后使用`html_element()`来查找将成为变量的元素。让\n我们通过一个简单的例子来看看这是如何操作的。这\n里我们有一个无序列表（`<ul>`），其中每个列表项（`<li>`）都包含《星球大战》中四个角色的信息：\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml <- minimal_html(\"\n  <ul>\n    <li><b>C-3PO</b> is a <i>droid</i> that weighs <span class='weight'>167 kg</span></li>\n    <li><b>R4-P17</b> is a <i>droid</i></li>\n    <li><b>R2-D2</b> is a <i>droid</i> that weighs <span class='weight'>96 kg</span></li>\n    <li><b>Yoda</b> weighs <span class='weight'>66 kg</span></li>\n  </ul>\n  \")\n```\n:::\n\n\n我们可以使用`html_elements()`创建一个向量，其中每个元素对应一个不同的字符:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncharacters <- html |> html_elements(\"li\")\ncharacters\n#> {xml_nodeset (4)}\n#> [1] <li>\\n<b>C-3PO</b> is a <i>droid</i> that weighs <span class=\"weight\"> ...\n#> [2] <li>\\n<b>R4-P17</b> is a <i>droid</i>\\n</li>\n#> [3] <li>\\n<b>R2-D2</b> is a <i>droid</i> that weighs <span class=\"weight\"> ...\n#> [4] <li>\\n<b>Yoda</b> weighs <span class=\"weight\">66 kg</span>\\n</li>\n```\n:::\n\n\n要提取每个角色的名字，我们使用`html_element()`，因为当应用于`html_elements()`的输出时，它保证每个元素返回一个响应:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncharacters |> html_element(\"b\")\n#> {xml_nodeset (4)}\n#> [1] <b>C-3PO</b>\n#> [2] <b>R4-P17</b>\n#> [3] <b>R2-D2</b>\n#> [4] <b>Yoda</b>\n```\n:::\n\n\n对于名字来说，`html_element()`和`html_elements()`之间的区别并不重要，但对于体重来说就很重要了。我\n们想要为每个角色获取一个体重，即使没有体重的`<span>`标签。这\n正是`html_element()`所做的：\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncharacters |> html_element(\".weight\")\n#> {xml_nodeset (4)}\n#> [1] <span class=\"weight\">167 kg</span>\n#> [2] NA\n#> [3] <span class=\"weight\">96 kg</span>\n#> [4] <span class=\"weight\">66 kg</span>\n```\n:::\n\n\n`html_elements()`查找所有作为`characters`子元素的体重`<span>`标签。但\n这样的标签只有三个，所以我们失去了名字和体重之间的关联：\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncharacters |> html_elements(\".weight\")\n#> {xml_nodeset (3)}\n#> [1] <span class=\"weight\">167 kg</span>\n#> [2] <span class=\"weight\">96 kg</span>\n#> [3] <span class=\"weight\">66 kg</span>\n```\n:::\n\n\n既然已经选择了感兴趣的元素，接下来需要从文本内容或某些属性中提取数据。\n\n### 文本和属性\n\n`html_text2()`[^webscraping-6] 提取HTML元素的纯文本内容:\n\n[^webscraping-6]: rvest 也提供了`html_text()`函数，但你几乎总是使用`html_text2()`，因为它在将嵌套的 HTML 转换为文本方面做得更好。\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncharacters |> \n  html_element(\"b\") |> \n  html_text2()\n#> [1] \"C-3PO\"  \"R4-P17\" \"R2-D2\"  \"Yoda\"\n\ncharacters |> \n  html_element(\".weight\") |> \n  html_text2()\n#> [1] \"167 kg\" NA       \"96 kg\"  \"66 kg\"\n```\n:::\n\n\n注意，任何转义都将被自动处理，你只会在源HTML中看到HTML转义，而不会在rvest返回的数据中看到。\n\n`html_attr()` 从属性中提取数据:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml <- minimal_html(\"\n  <p><a href='https://en.wikipedia.org/wiki/Cat'>cats</a></p>\n  <p><a href='https://en.wikipedia.org/wiki/Dog'>dogs</a></p>\n\")\n\nhtml |> \n  html_elements(\"p\") |> \n  html_element(\"a\") |> \n  html_attr(\"href\")\n#> [1] \"https://en.wikipedia.org/wiki/Cat\" \"https://en.wikipedia.org/wiki/Dog\"\n```\n:::\n\n\n`html_attr()` 总是返回一个字符串，所以如果要提取数字或日期，需要做一些后处理。\n\n### 表格\n\n如果你很幸运，你的数据已经存储在 HTML 表格中，那么只需要从该表格中读取数据即可。在\n浏览器中识别表格通常很简单：它会有一个由行和列组成的矩形结构，你可以将其复制并粘贴到像 Excel 这样的工具中。\n\nHTML 表格由四个主要元素组成：`<table>` (表格)、`<tr>` (表格行)、`<th>` (表格标题)和 `<td>` (表格数据)。下\n面是一个包含两列三行的简单 HTML 表格示例：\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml <- minimal_html(\"\n  <table class='mytable'>\n    <tr><th>x</th>   <th>y</th></tr>\n    <tr><td>1.5</td> <td>2.7</td></tr>\n    <tr><td>4.9</td> <td>1.3</td></tr>\n    <tr><td>7.2</td> <td>8.1</td></tr>\n  </table>\n  \")\n```\n:::\n\n\nrvest 提供了一个`html_table()`函数来读取这种类型的数据。它\n返回一个列表，其中包含页面上找到的每个表格的一个 tibble。使\n用`html_element()`来识别你想要提取的表格：\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml |> \n  html_element(\".mytable\") |> \n  html_table()\n#> # A tibble: 3 × 2\n#>       x     y\n#>   <dbl> <dbl>\n#> 1   1.5   2.7\n#> 2   4.9   1.3\n#> 3   7.2   8.1\n```\n:::\n\n\n请注意，`x` 和 `y` 已经自动被转换为数字。这\n种自动转换并不总是有效，因此在更复杂的场景中，你可能想通过设置 `convert = FALSE` 来关闭它，然后进行自己的转换。\n\n## 找到正确的选择器 {#sec-css-selectors}\n\n确定数据对应的选择器通常是这个问题中最困难的部分。你\n通常需要做一些尝试来找到一个既特异（即不选择你不关心的内容）又敏感（即选择你关心的所有内容）的选择器。大\n量的尝试和错误是这个过程的正常部分！有\n两个主要工具可以帮助你完成这个过程：SelectorGadget 和浏览器的开发者工具。\n\n[SelectorGadget](https://rvest.tidyverse.org/articles/selectorgadget.html) 是一个 JavaScript 书签工具，它基于你提供的正面和反面示例来自动生成 CSS 选择器。它\n并不总是有效，但当它成功时，它就像神奇魔法一样！你\n可以通过阅读 <https://rvest.tidyverse.org/articles/selectorgadget.html> 或观看 Mine 的视频 [https://www.youtube.com/watch?v=PetWV5g1Xsc](#0) 来学习如何安装和使用 SelectorGadget。\n\n每个现代浏览器都带有一些开发者工具包，但我们推荐使用 Chrome，即使它不是你的常用浏览器。它\n的网络开发者工具是最好的，而且立即可用。在\n页面上的一个元素上右键单击并选择`Inspect`，这将打开一个可扩展的完整 HTML 页面视图，以你刚刚点击的元素为中心。你\n可以使用它来探索页面并了解哪些选择器可能有效。特\n别注意 class 和 id 属性，因为这些属性通常用于形成页面的视觉结构，因此是提取你正在寻找的数据的好工具。\n\n在“元素”视图中，你还可以右键单击一个元素并选择`Copy as Selector`以生成一个将唯一标识感兴趣元素的选择器。\n\n如果 SelectorGadget 或 Chrome DevTools 生成了你不理解的 CSS 选择器，请尝试使用 [Selectors Explained](https://kittygiraudel.github.io/selectors-explained/){.uri}，它将 CSS 选择器翻译成简单的英语。如\n果你发现自己经常这样做，你可能想要更深入地了解 CSS 选择器。我\n们推荐你从有趣的 [CSS dinner](https://flukeout.github.io/) 教程开始，然后参考 [MDN web docs](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Selectors)。\n\n## 整合\n\n让我们将这些内容整合起来以抓取一些网站。这\n些示例在你运行它们时可能不再有效，这是网页抓取的基本挑战；如果网站的结构发生变化，那么你就需要更改你的抓取代码。\n\n### 星球大战\n\nrvest在`vignette(\"starwars\")`中包含一个非常简单的示例。这\n是一个简单的页面，HTML 内容很少，所以它是一个很好的起点。我\n鼓励你现在就导航到那个页面，并使用\"Inspect Element\" 来检查一个标题，该标题是星球大战电影的名称。使\n用键盘或鼠标探索 HTML 的层次结构，看看你是否能感觉到每部电影使用的共享结构。\n\n你应该能够看到每部电影都有一个共享的结构，看起来像这样：\n\n``` html\n<section>\n  <h2 data-id=\"1\">The Phantom Menace</h2>\n  <p>Released: 1999-05-19</p>\n  <p>Director: <span class=\"director\">George Lucas</span></p>\n  \n  <div class=\"crawl\">\n    <p>...</p>\n    <p>...</p>\n    <p>...</p>\n  </div>\n</section>\n```\n\n我们的目标是将这些数据转换为一个包含7行的数据框，变量包括 `title`、`year`、`director`和 `intro`。首\n先，我们将读取HTML并提取所有的`<section>`元素：\n\n\n::: {.cell}\n\n```{.r .cell-code}\nurl <- \"https://rvest.tidyverse.org/articles/starwars.html\"\nhtml <- read_html(url)\n\nsection <- html |> html_elements(\"section\")\nsection\n#> {xml_nodeset (7)}\n#> [1] <section><h2 data-id=\"1\">\\nThe Phantom Menace\\n</h2>\\n<p>\\nReleased: 1 ...\n#> [2] <section><h2 data-id=\"2\">\\nAttack of the Clones\\n</h2>\\n<p>\\nReleased: ...\n#> [3] <section><h2 data-id=\"3\">\\nRevenge of the Sith\\n</h2>\\n<p>\\nReleased:  ...\n#> [4] <section><h2 data-id=\"4\">\\nA New Hope\\n</h2>\\n<p>\\nReleased: 1977-05-2 ...\n#> [5] <section><h2 data-id=\"5\">\\nThe Empire Strikes Back\\n</h2>\\n<p>\\nReleas ...\n#> [6] <section><h2 data-id=\"6\">\\nReturn of the Jedi\\n</h2>\\n<p>\\nReleased: 1 ...\n#> [7] <section><h2 data-id=\"7\">\\nThe Force Awakens\\n</h2>\\n<p>\\nReleased: 20 ...\n```\n:::\n\n\n这获取了与页面上找到的七部电影相匹配的七个元素，这表明使用`section`作为选择器是合适的。由\n于数据总是在文本中找到，提取单个元素是很直接的，只是需要找到正确的选择器：\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsection |> html_element(\"h2\") |> html_text2()\n#> [1] \"The Phantom Menace\"      \"Attack of the Clones\"   \n#> [3] \"Revenge of the Sith\"     \"A New Hope\"             \n#> [5] \"The Empire Strikes Back\" \"Return of the Jedi\"     \n#> [7] \"The Force Awakens\"\n\nsection |> html_element(\".director\") |> html_text2()\n#> [1] \"George Lucas\"     \"George Lucas\"     \"George Lucas\"    \n#> [4] \"George Lucas\"     \"Irvin Kershner\"   \"Richard Marquand\"\n#> [7] \"J. J. Abrams\"\n```\n:::\n\n\n一旦我们对每个组件都完成了这些操作，我们就可以将所有结果包装成一个tibble：\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(\n  title = section |> \n    html_element(\"h2\") |> \n    html_text2(),\n  released = section |> \n    html_element(\"p\") |> \n    html_text2() |> \n    str_remove(\"Released: \") |> \n    parse_date(),\n  director = section |> \n    html_element(\".director\") |> \n    html_text2(),\n  intro = section |> \n    html_element(\".crawl\") |> \n    html_text2()\n)\n#> # A tibble: 7 × 4\n#>   title                   released   director         intro                  \n#>   <chr>                   <date>     <chr>            <chr>                  \n#> 1 The Phantom Menace      1999-05-19 George Lucas     \"Turmoil has engulfed …\n#> 2 Attack of the Clones    2002-05-16 George Lucas     \"There is unrest in th…\n#> 3 Revenge of the Sith     2005-05-19 George Lucas     \"War! The Republic is …\n#> 4 A New Hope              1977-05-25 George Lucas     \"It is a period of civ…\n#> 5 The Empire Strikes Back 1980-05-17 Irvin Kershner   \"It is a dark time for…\n#> 6 Return of the Jedi      1983-05-25 Richard Marquand \"Luke Skywalker has re…\n#> # ℹ 1 more row\n```\n:::\n\n\n我们对`released`进行了一些额外的处理，以便得到一个变量，这个变量将在我们后续的分析中更容易使用。\n\n### IMDB 顶级电影\n\n对于下一个任务，我们将解决一个稍微棘手一点的问题，即从互联网电影数据库（IMDb）中提取前250部电影。在\n我们撰写本章时，该页面看起来像这样 @fig-scraping-imdb 。\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Screenshot of the IMDb top movies web page taken on 2022-12-05.\n](screenshots/scraping-imdb.png){#fig-scraping-imdb fig-alt='The screenshot shows a table with columns \"Rank and Title\",\n\"IMDb Rating\", and \"Your Rating\". 9 movies out of the top 250\nare shown. The top 5 are the Shawshank Redemption, The Godfather,\nThe Dark Knight, The Godfather: Part II, and 12 Angry Men.' width=418}\n:::\n:::\n\n\n这些数据有一个清晰的表格结构，所以值得从`html_table()`开始:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# url <- \"https://web.archive.org/web/20220201012049/https://www.imdb.com/chart/top/\"\n# html <- read_html(url)\n# \n# table <- html |> \n#   html_element(\"table\") |> \n#   html_table()\n# table\n```\n:::\n\n\n这包括一些空列，但总体上很好地捕获了表格中的信息。然\n而，为了使它更容易使用，我们需要进行更多的处理。首\n先，我们将重命名列以便于操作，并删除排名和标题中的多余空格。我\n们将使用`select()`（而不是`rename()`）来一步完成这两个列的重命名和选择。然\n后我们将删除换行符和多余的空格，接着使用`separate_wider_regex()`（来自 @sec-extract-variables ）来将标题、年份和排名提取到它们自己的变量中。\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ratings <- table |>\n#   select(\n#     rank_title_year = `Rank & Title`,\n#     rating = `IMDb Rating`\n#   ) |> \n#   mutate(\n#     rank_title_year = str_replace_all(rank_title_year, \"\\n +\", \" \")\n#   ) |> \n#   separate_wider_regex(\n#     rank_title_year,\n#     patterns = c(\n#       rank = \"\\\\d+\", \"\\\\. \",\n#       title = \".+\", \" +\\\\(\",\n#       year = \"\\\\d+\", \"\\\\)\"\n#     )\n#   )\n# ratings\n```\n:::\n\n\n即使在这种情况下，大部分数据都来自表格单元格，查看原始HTML仍然是有价值的。如\n果你这样做，你会发现我们可以利用其中一个属性来添加一些额外的数据。这\n正是值得花一点时间去探索页面源代码的原因之一；你可能会发现额外的数据，或者找到稍微简单一些的解析路径。\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml |> \n  html_elements(\"td strong\") |> \n  head() |> \n  html_attr(\"title\")\n#> character(0)\n```\n:::\n\n\n我们可以将这与表格数据结合起来，并再次应用`separate_wider_regex()`函数来提取我们关心的数据部分：\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ratings |>\n#   mutate(\n#     rating_n = html |> html_elements(\"td strong\") |> html_attr(\"title\")\n#   ) |> \n#   separate_wider_regex(\n#     rating_n,\n#     patterns = c(\n#       \"[0-9.]+ based on \",\n#       number = \"[0-9,]+\",\n#       \" user ratings\"\n#     )\n#   ) |> \n#   mutate(\n#     number = parse_number(number)\n#   )\n```\n:::\n\n\n## 动态网站\n\n到目前为止，我们主要关注的是那些`html_elements()`返回你在浏览器中看到的内容的网站，并讨论了如何解析这些返回的内容以及如何在整齐的数据框中组织这些信息。然\n而，有时你会遇到`html_elements()`及其相关函数返回的内容与你在浏览器中看到的内容截然不同的情况。在\n很多情况下，这是因为你试图抓取的是使用javascript动态生成页面内容的网站。这\n在目前不能与`rvest`一起工作，因为`rvest`下载的是原始的HTML，并不运行任何javascript。\n\n尽管如此，仍然有可能抓取这些类型的网站，但`rvest`需要使用一个更复杂的过程：完全模拟网络浏览器，包括运行所有的javascript。在\n撰写本文时，这一功能尚未可用，但这是我们正在积极开发的内容，并且可能在你阅读本文时已经可用。它\n使用[chromote](https://rstudio.github.io/chromote/index.html) 包，该包实际上在后台运行Chrome浏览器，并为你提供了与网站交互的额外工具，就像人类一样输入文本和点击按钮。更\n多详细信息，请查看[rvest website](http://rvest.tidyverse.org/)。\n\n## 小结\n\n在本章中，你学习了从网页抓取数据的原因、为什么不可以以及如何抓取。首\n先，学习了HTML的基础知识以及如何使用CSS选择器来引用特定的元素，然后学习了使用`rvest`包将HTML中的数据导入R。接\n着，我们通过两个案例研究来展示网页抓取：一个更简单的场景是从`rvest`包网站抓取关于《星球大战》电影的数据，以及一个更复杂的场景是从IMDB抓取前250部电影的数据。\n\n从网页上抓取数据的技术细节可能会很复杂，尤其是当处理动态生成的网站时，但法律和道德考虑可能更为复杂。在\n着手抓取数据之前，你需要对这两方面都有所了解。\n\n你已经学习了将数据从其原始位置（电子表格、数据库、JSON文件和网站）以整齐的形式导入R中的技术，这标志着我们完成了本书中关于数据导入的内容。现\n在，是时候将目光投向一个新的主题了: 充分利用R作为一种编程语言。\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}