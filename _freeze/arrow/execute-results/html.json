{
  "hash": "778459774f5aca1b9b422e9761dc6981",
  "result": {
    "engine": "knitr",
    "markdown": "---\nfreeze: true\n---\n\n\n# Arrow {#sec-arrow}\n\n\n::: {.cell}\n\n:::\n\n\n## 引言\n\nCSV 文件设计得易于人类阅读。它\n们是一种很好的交换格式，因为它们非常简单，并且可以被任何工具读取。但\n是 CSV 文件并不是非常高效，你需要做很多工作才能将数据读入 R。在\n本章中，你将了解一个强大的替代方案：[parquet](https://parquet.apache.org/) 格式，这是一种基于开放标准的大数据系统广泛使用的格式。\n\n我们将把 parquet 文件与 [Apache Arrow](https://arrow.apache.org) 配对使用，Apache Arrow 是一个为大型数据集的高效分析和传输而设计的多语言工具箱。我\n们将通过 [arrow](https://arrow.apache.org/docs/r/) 包来使用 Apache Arrow，它提供了一个 dplyr 后端，允许你使用熟悉的 dplyr 语法来分析大于内存的数据集。作\n为一个额外的优势，arrow 极其快速：你将在本章后面看到一些示例。\n\narrow 和 dbplyr 都提供了 dplyr 后端，所以你可能会想知道什么时候使用哪一个。在\n许多情况下，这个选择已经为你做出了，因为数据已经存储在数据库中或 parquet 文件中，你会想要保持现状进行工作。但\n是如果你从自己的数据（可能是 CSV 文件）开始，你可以将其加载到数据库中或将其转换为 parquet。一\n般来说，很难知道哪种方法效果最好，所以在分析的早期阶段，我们鼓励你尝试两者，并选择最适合你的那一种。\n\n（特别感谢 Danielle Navarro 贡献了本章的初始版本。）\n\n### 必要条件\n\n在本章中，我们将继续使用 tidyverse，特别是 dplyr，但我们将它与 arrow 包结合使用，arrow 包是专门为处理大数据而设计的。\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(arrow)\n```\n:::\n\n\n在本章的后面部分，我们还将看到 arrow 和 duckdb 之间的一些联系，因此我们还需要使用 dbplyr 和 duckdb。\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dbplyr, warn.conflicts = FALSE)\nlibrary(duckdb)\n#> Loading required package: DBI\n```\n:::\n\n\n## 获取数据\n\n我们首先获取一个值得使用这些工具的数据集：西雅图公共图书馆的图书借阅数据集，可以在线获取，地址为 [data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6](https://data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6)。这\n个数据集包含了 41,389,465 行数据，记录了从 2005 年 4 月到 2022 年 10 月每个月每本书的借阅次数。\n\n以下代码将为你获取数据的缓存副本。数\n据是一个 9GB 的 CSV 文件，因此下载需要一些时间。我\n强烈推荐使用 `curl::multi_download()` 来获取非常大的文件，因为它正是为此目的而构建的：它提供了一个进度条，并且如果下载中断，它可以恢复下载。\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndir.create(\"data\", showWarnings = FALSE)\n\ncurl::multi_download(\n  \"https://r4ds.s3.us-west-2.amazonaws.com/seattle-library-checkouts.csv\",\n  \"data/seattle-library-checkouts.csv\",\n  resume = TRUE\n)\n```\n:::\n\n\n## 打开数据集\n\n首先来查看一下数据。由\n于该文件有 9GB，足够大，因此我们可能不想将整个文件加载到内存中。一\n个很好的经验法则是，你通常希望至少有数据大小两倍的内存，而许多笔记本电脑的内存容量上限为16GB。这\n意味着我们要避免使用 `read_csv()`，而是使用 `arrow::open_dataset()`：\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv <- open_dataset(\n  sources = \"data/seattle-library-checkouts.csv\", \n  col_types = schema(ISBN = string()),\n  format = \"csv\"\n)\n```\n:::\n\n\n运行这段代码时发生了什么？`o`\n`pen_dataset()` 会扫描几千行数据以确定数据集的结构。`I`\n`SBN` 列在前 80,000 行中包含空值，因此我们必须指定列类型以帮助 arrow 确定数据结构。一\n旦 `open_dataset()` 扫描了数据，它就会记录所发现的内容并停止；只有当你特别请求时，它才会进一步读取其他行。如\n果我们输出`seattle_csv`，所看到的正是这些元数据。\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv\n#> FileSystemDataset with 1 csv file\n#> UsageClass: string\n#> CheckoutType: string\n#> MaterialType: string\n#> CheckoutYear: int64\n#> CheckoutMonth: int64\n#> Checkouts: int64\n#> Title: string\n#> ISBN: string\n#> Creator: string\n#> Subjects: string\n#> Publisher: string\n#> PublicationYear: string\n```\n:::\n\n\n输出的第一行告诉你 `seattle_csv` 作为单个 CSV 文件存储在本地磁盘上；它只会在需要时加载到内存中。输\n出的其余部分告诉你 arrow 为每一列推断出的列类型。\n\n我们可以使用 `glimpse()` 来查看实际内容。这\n会显示大约有 4100 万行和 12 列，并展示一些值。\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv |> glimpse()\n#> FileSystemDataset with 1 csv file\n#> 129,967 rows x 12 columns\n#> $ UsageClass      <string> \"Physical\", \"Physical\", \"Digital\", \"Physical\", \"Ph…\n#> $ CheckoutType    <string> \"Horizon\", \"Horizon\", \"OverDrive\", \"Horizon\", \"Hor…\n#> $ MaterialType    <string> \"BOOK\", \"BOOK\", \"EBOOK\", \"BOOK\", \"SOUNDDISC\", \"BOO…\n#> $ CheckoutYear     <int64> 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 20…\n#> $ CheckoutMonth    <int64> 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,…\n#> $ Checkouts        <int64> 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 2, 3, 2, 1, 3, 2,…\n#> $ Title           <string> \"Super rich : a guide to having it all / Russell S…\n#> $ ISBN            <string> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n#> $ Creator         <string> \"Simmons, Russell\", \"Barclay, James, 1965-\", \"Tim …\n#> $ Subjects        <string> \"Self realization, Conduct of life, Attitude Psych…\n#> $ Publisher       <string> \"Gotham Books,\", \"Pyr,\", \"Random House, Inc.\", \"Di…\n#> $ PublicationYear <string> \"c2011.\", \"2010\", \"2015\", \"2005\", \"c2004.\", \"c2005…\n```\n:::\n\n\n我们可以开始使用 dplyr 的函数来操作这个数据集，使用 `collect()` 来强制 arrow 执行计算并返回一些数据。例\n如，以下代码会告诉我们每年图书的总借阅次数：\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv |> \n  group_by(CheckoutYear) |> \n  summarise(Checkouts = sum(Checkouts)) |> \n  arrange(CheckoutYear) |> \n  collect()\n#> # A tibble: 2 × 2\n#>   CheckoutYear Checkouts\n#>          <int>     <int>\n#> 1         2016    411739\n#> 2         2022         1\n```\n:::\n\n\n由于使用了arrow，这段代码无论基础数据集有多大都能正常工作。但\n目前的运行速度相当慢：在 Hadley 的电脑上，运行这段代码大约需要 10 秒。考\n虑到我们处理的数据量，这不算太慢，但我们可以通过切换到更好的格式来使其更快。\n\n## parquet 格式 {#sec-parquet}\n\n为了让这些数据更易于处理，我们将切换到 parquet 文件格式，并将其拆分成多个文件。接\n下来的部分将首先向你介绍 parquet 和分区，然后将我们学到的知识应用于西雅图图书馆数据。\n\nTo make this data easier to work with, let's switch to the parquet file format and split it up into multiple files.\nThe following sections will first introduce you to parquet and partitioning, and then apply what we learned to the Seattle library data.\n\n### parquet 的优势\n\n和 CSV 一样，parquet 用于矩形数据，但它不是任何文件编辑器都可以读取的文本格式，而是一个专门为大数据需求设计的自定义二进制格式。这\n意味着：\n\n-   parquet 文件通常比等效的 CSV 文件小。Parquet 依赖于[efficient encodings](https://parquet.apache.org/docs/file-format/data-pages/encodings/)来减小文件大小，并支持文件压缩。这有助于使 parquet 文件更快，因为从磁盘移动到内存的数据更少。\n-   parquet 文件具有丰富的类型系统。正如我们在 @sec-col-types 讨论的那样，CSV 文件不提供有关列类型的任何信息。例如CSV 阅读器必须猜测 `\"08-10-2022\"` 是否应被解析为字符串或日期。相反，parquet 文件以将数据与其类型一起记录的方式存储数据。\n-   parquet 文件是“面向列的”。这意味着它们是按列组织的，类似于 R 的数据框。与按行组织的 CSV 文件相比，这通常会导致数据分析任务具有更好的性能。\n-   parquet 文件是“分块的”，这使得可以在同一时间处理文件的不同部分，如果幸运的话，甚至可以完全跳过一些块。\n\nParquet 文件有一个主要缺点：它们不再是“人类可读”的，即如果你使用 `readr::read_file()` 查看 Parquet 文件，你将只能看到一堆乱码。\n\n### 分区\n\n随着数据集变得越来越大，将所有数据存储在一个文件中变得越来越困难，而且将大型数据集拆分成多个文件通常很有用。当\n这种结构化操作进行得足够智能时，这种策略可以显著提高性能，因为许多分析只需要文件的一个子集。\n\n关于如何对你的数据集进行分区，没有严格的规定：结果将取决于你的数据、访问模式以及读取数据的系统。在\n找到适合你的理想分区之前，可能需要进行一些实验。作\n为粗略的指导，arrow 建议避免使用小于 20MB 和大于 2GB 的文件，并避免产生超过 10,000 个文件的分区。你\n还应该尝试按你筛选的变量进行分区；如你稍后将看到的那样，这允许 arrow 仅读取相关文件，从而节省大量工作。\n\n### 重写西雅图图书馆数据\n\n让我们将这些想法应用于西雅图图书馆数据，以了解它们在实际操作中的表现。我\n们将按 `CheckoutYear` 进行分区，因为某些分析可能只想查看最近的数据，并且按年份分区可以产生 18 个大小合理的块。\n\n为了重写数据，我们使用 `dplyr::group_by()` 定义分区，然后使用 `arrow::write_dataset()` 将分区保存到目录中。`w`\n`rite_dataset()` 有两个重要参数：一个是用于创建文件的目录，一个是将使用的文件格式。\n\n\n::: {.cell}\n\n```{.r .cell-code}\npq_path <- \"data/seattle-library-checkouts\"\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv |>\n  group_by(CheckoutYear) |>\n  write_dataset(path = pq_path, format = \"parquet\")\n```\n:::\n\n\n这大约需要一分钟来运行；正如我们稍后将看到的，这是一项初始投资，通过使未来的操作变得更快而得到回报。\n\n让我们来看看刚刚生成了什么：\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(\n  files = list.files(pq_path, recursive = TRUE),\n  size_MB = file.size(file.path(pq_path, files)) / 1024^2\n)\n#> # A tibble: 2 × 2\n#>   files                             size_MB\n#>   <chr>                               <dbl>\n#> 1 CheckoutYear=2016/part-0.parquet 14.0    \n#> 2 CheckoutYear=2022/part-0.parquet  0.00434\n```\n:::\n\n\n我们原本 9GB 的 CSV 文件已经被重写为 18 个 parquet 文件。这\n些文件名采用了 [Apache Hive](https://hive.apache.org)项目使用的“自描述”约定。H\nive 风格的分区使用“key=value”约定来命名文件夹，所以正如你猜测的那样，名为 `CheckoutYear=2005` 的目录包含了所有 `CheckoutYear` 为 2005 的数据。每\n个文件的大小在 100MB 到 300MB 之间，总大小现在大约是 4GB，略多于原始 CSV 文件大小的一半。这\n是我们所期望的，因为 parquet 是一种更加高效的格式。\n\n## 使用 dplyr 与 arrow\n\n现在我们已经创建了这些 parquet 文件，接下来我们需要再次读取它们。我\n们再次使用 `open_dataset()`，但这次我们给它提供一个目录：\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_pq <- open_dataset(pq_path)\n```\n:::\n\n\n现在我们可以编写我们的 dplyr 管道了。例\n如，我们可以计算过去五年内每个月借出的书籍总数：\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquery <- seattle_pq |> \n  filter(CheckoutYear >= 2018, MaterialType == \"BOOK\") |>\n  group_by(CheckoutYear, CheckoutMonth) |>\n  summarize(TotalCheckouts = sum(Checkouts)) |>\n  arrange(CheckoutYear, CheckoutMonth)\n```\n:::\n\n\n为 arrow 数据编写 dplyr 代码的概念与 @sec-import-databases 中的dbplyr 类似：你编写 dplyr 代码，这些代码会自动转换为 Apache Arrow C++ 库能够理解的查询，然后在调用 `collect()` 时执行。如\n果我们输出查询对象，我们可以看到一些关于执行时我们期望 Arrow 返回的信息：\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquery\n#> FileSystemDataset (query)\n#> CheckoutYear: int32\n#> CheckoutMonth: int64\n#> TotalCheckouts: int64\n#> \n#> * Grouped by CheckoutYear\n#> * Sorted by CheckoutYear [asc], CheckoutMonth [asc]\n#> See $.data for the source Arrow object\n```\n:::\n\n\n我们可以通过调用`collect()`来获取结果:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquery |> collect()\n#> # A tibble: 0 × 3\n#> # Groups:   CheckoutYear [0]\n#> # ℹ 3 variables: CheckoutYear <int>, CheckoutMonth <int>,\n#> #   TotalCheckouts <int>\n```\n:::\n\n\n与 dbplyr 一样，arrow 只理解某些 R 表达式，因此你可能无法编写与通常完全一样的代码。但\n是，所支持的操作和函数列表相当广泛且不断扩展；您可以在`?acero`中找到当前支持的函数的完整列表。\n\n### 性能 {#sec-parquet-fast}\n\n让我们快速看一下从 CSV 切换到 parquet 对性能的影响。首\n先，让我们计算一下当数据作为单个大型 CSV 存储时，计算 2021 年每个月借出的书籍数量所需的时间：\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv |> \n  filter(CheckoutYear == 2021, MaterialType == \"BOOK\") |>\n  group_by(CheckoutMonth) |>\n  summarize(TotalCheckouts = sum(Checkouts)) |>\n  arrange(desc(CheckoutMonth)) |>\n  collect() |> \n  system.time()\n#>    user  system elapsed \n#>   0.065   0.004   0.065\n```\n:::\n\n\n现在，让我们使用数据集的新版本，其中西雅图图书馆的借阅数据已经被分区为 18 个较小的 parquet 文件：\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_pq |> \n  filter(CheckoutYear == 2021, MaterialType == \"BOOK\") |>\n  group_by(CheckoutMonth) |>\n  summarize(TotalCheckouts = sum(Checkouts)) |>\n  arrange(desc(CheckoutMonth)) |>\n  collect() |> \n  system.time()\n#>    user  system elapsed \n#>   0.018   0.002   0.022\n```\n:::\n\n\n大约100倍性能的提升可归因于两个因素：多文件分区和单个文件的格式：\n\n-   分区提高了性能，因为这个查询使用 `CheckoutYear == 2021` 来筛选数据，而 arrow 足够智能，能够识别出它只需要读取 18 个 parquet 文件中的 1 个。\n-   parquet 格式提高了性能，因为它将数据以可以直接读入内存的二进制格式存储。列式格式和丰富的元数据意味着 arrow 只需要读取查询中实际使用的四列（`CheckoutYear`、`MaterialType`、`CheckoutMonth` 和 `Checkouts`）。\n\n这种性能上的巨大差异说明为什么将大型 CSV 转换为 parquet 是值得的！\n\n### 使用duckdb与arrow\n\nparquet 和 arrow 的最后一个优势是，通过调用`arrow::to_duckdb()`可以非常轻松地将arrow 数据集转换为 DuckDB 数据库（ 参见 @sec-import-databases ）。\n\nThere's one last advantage of parquet and arrow --- it's very easy to turn an arrow dataset into a DuckDB database (@sec-import-databases) by calling `arrow::to_duckdb()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_pq |> \n  to_duckdb() |>\n  filter(CheckoutYear >= 2018, MaterialType == \"BOOK\") |>\n  group_by(CheckoutYear) |>\n  summarize(TotalCheckouts = sum(Checkouts)) |>\n  arrange(desc(CheckoutYear)) |>\n  collect()\n#> Warning: Missing values are always removed in SQL aggregation functions.\n#> Use `na.rm = TRUE` to silence this warning\n#> This warning is displayed once every 8 hours.\n#> # A tibble: 0 × 2\n#> # ℹ 2 variables: CheckoutYear <int>, TotalCheckouts <dbl>\n```\n:::\n\n\n`to_duckdb()`的巧妙之处在于，这种转换不涉及任何内存复制，并体现了 arrow 生态系统的目标：实现从一个计算环境到另一个计算环境的无缝过渡。\n\n### 练习\n\n1.  找出每年最受欢迎的书。\n2.  西雅图图书馆系统中哪位作者的书最多？\n3.  在过去的10年里，纸质书与电子书的借阅情况是如何变化的？\n\n## 小结\n\n在本章中，你已经初步了解了arrow包，它提供了用于处理大型磁盘上数据集的 dplyr 后端。它\n可以处理 CSV 文件，但如果您将数据转换为 parquet 格式，它会快得多。P\narquet 是一种二进制数据格式，专为现代计算机上的数据分析而设计。与\nCSV 相比，能够处理 parquet 文件的工具要少得多，但其分区、压缩和列式存储结构使得分析更加高效。\n\n接下来，您将学习关于第一个非矩形数据源的知识，您将使用 tidyr 包提供的工具来处理它。我\n们将重点关注来自 JSON 文件的数据，但一般原则适用于任何来源的树状数据。\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}